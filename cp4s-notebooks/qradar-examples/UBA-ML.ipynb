{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML models for large feature set in one AQL query\n",
    "\n",
    "Goal of this notebook is to try making some models from 70+ features extracted from one bit QRadar query.  Each row is an entry for a user (plus a time slice).\n",
    "\n",
    "The features are broken into a few different models. See the large query AQL below for all the features.  It would also be possible to build a seperate model for each individual feature but that could be very noisy or generate a lot of alerts.  I would propose a model for related features like:\n",
    "- General are general QRadar features like number of qid, events, log sources, devices, context\n",
    "- Time is time based features like min/max/avg, events exactly on the start of hour and start of each minute\n",
    "- Network is things related to the network addresses like IP, local/remote, mac addresses\n",
    "- Port is the traffic in ranges 0 - 1024 - 49151 - max, plus unique ports in each range\n",
    "- Rules is info around QRadar and UBA BB, like unique rules, UBA risk\n",
    "- 'All Columns' is all 70+ features at once\n",
    "\n",
    "Ideas for other models:\n",
    "- Proxy: things like unique URLs, http/https traffic, URL categories, source and dest IPs\n",
    "- Windows: object name, types, domain, eventID, process nametc etc.\n",
    "- UNIX model\n",
    "- Cloud: AWS, azure, office 365.  Things like how many EC2 instances, how many files, cloud object storage, how many S3 buckets accessed\n",
    "- Authentication model - normal auth times, amount of auth, auth device (VPN/domain controller etc), auth source IP's\n",
    "\n",
    "The first model is for an entire population, then look at some sample users to check if they are an inlier or outlier.  So this checks how close or similar each person is compared to everyone.  For example if someone's 'Port' features are very different from the rest of the peers 'Port' features, then they would be marked an outlier.  This same model could be used to look at peer groups instead of the whole population.  So we could make a model for each department, city, job title etc.  Example: make a model for city 'Sandy Springs', then for each person in Sandy Springs see if they are an outlier for each model (Port, Proxy, Network etc).\n",
    "\n",
    "The next model I looked at was just a person versus their own historical data.  So take a week of data, and check the latest point to see if it is an inlier or outlier.  This would determine if their behavior changed for a model vs themself in the past.  For example we could make a model for Proxy features, and see if my Proxy features today or this hour are different from my past ones.\n",
    "\n",
    "Advantanges:\n",
    "- one big query for all data at once is much more efficient. See: https://github.ibm.com/infosec/uba/issues/4203#issuecomment-12407381\n",
    "- have all features at once.  Can then use for multiple models and views\n",
    "- **view:** vs self, vs whole population, vs peers (department, city, job title)\n",
    "- **model:** can use all features, subsets of features (port/proxy/IP etc.), or one by one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default settings, constants\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "FIGSIZE=(15,10)\n",
    "LABEL = 'label'\n",
    "FAMILY = 'family'\n",
    "RANDOM=0\n",
    "SPLIT=0.2\n",
    "\n",
    "SCALE_DATA = False\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = FIGSIZE\n",
    "\n",
    "USERS = [\n",
    "    'admin',\n",
    "    'root',\n",
    "    'DeptMgrAll-5057',\n",
    "    'testuser-31176',\n",
    "    'svc_emon',\n",
    "    'configservices',\n",
    "    'MATTO'\n",
    "]\n",
    "\n",
    "# Prefixes for ariel results\n",
    "PREFIX = [\n",
    "    'General',\n",
    "    'Time',\n",
    "    'Network',\n",
    "    'Port',\n",
    "    'Rules',\n",
    "    'All Columns'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qradar import QRadar, AQL\n",
    "\n",
    "qi = QRadar(console='9.191.82.171', username='admin', token='YOUR-SERVICE-TOKEN-HERE')\n",
    "df = pd.DataFrame.from_records(qi.search(AQL.proxy_model))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import  MinMaxScaler, StandardScaler, RobustScaler\n",
    "import time\n",
    "\n",
    "def mean(l):\n",
    "    return float(sum(l))/float(len(l))\n",
    "\n",
    "def test_model(prefix='All Columns'):\n",
    "    print('Model for %s' % prefix)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    if prefix == 'All Columns':\n",
    "        data = df\n",
    "    else:\n",
    "        cols = ['user', 'timeslice']\n",
    "        cols.extend([col for col in df if col.startswith(prefix.lower()+'_')])\n",
    "        data = df[cols]\n",
    "        \n",
    "    # Scale data\n",
    "    if SCALE_DATA:\n",
    "        numeric_col = data.columns[data.dtypes.apply(lambda c: np.issubdtype(c, np.number))]\n",
    "        scaler = RobustScaler() \n",
    "        scaled_values = scaler.fit_transform(data[numeric_col]) \n",
    "        data[numeric_col] = scaled_values\n",
    "\n",
    "    features=data.drop('user',axis=1).drop('timeslice',axis=1)\n",
    "    print('%s feature columns' % len(list(features)))\n",
    "    \n",
    "    isof = IsolationForest(behaviour='new', contamination='auto', n_jobs=-1)\n",
    "    try:\n",
    "        isof.fit(features)\n",
    "    except:\n",
    "        return  # skip the rest\n",
    "    \n",
    "    for username in USERS:\n",
    "        sample = data[data['user'] == username].drop('user',axis=1).drop('timeslice',axis=1)\n",
    "        if sample.empty:\n",
    "            continue\n",
    "        isof_pred = isof.predict(sample)\n",
    "        isof_dec = isof.decision_function(sample) # > 0 normal, < 0 outlier\n",
    "        isof_score = isof.score_samples(sample) # lower more abnormal\n",
    "        \n",
    "        pred[username].append(mean(isof_pred))\n",
    "        dec[username].append(mean(isof_dec))\n",
    "        score[username].append(mean(isof_score))\n",
    "    print('took %.2f seconds' % (time.time() - start))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model each user vs entire population\n",
    "\n",
    "This uses the whole population to make a model, and looks for outliers vs the general population per model.\n",
    "\n",
    "You could use the same idea or code, but instead of the whole population, key off of some other attribute like job title, city, department.  So do this similar model and instead of comparing against everyone, compare against the same city 'Fredericton' and so on for each value.\n",
    "\n",
    "I try a big general model using all the features.  Then also some models using a sub-set of features like just IP ones, just port ones, just proxy features etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "pred = defaultdict(list)\n",
    "dec = defaultdict(list)\n",
    "score = defaultdict(list)\n",
    "\n",
    "for prefix in PREFIX:\n",
    "    test_model(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(d, label):\n",
    "    print(label)\n",
    "    for key in d:\n",
    "        print('%s: %s' % (key, [ '%.2f' % i for i in d[key] ]))\n",
    "    print('')\n",
    "\n",
    "    \n",
    "print(', '.join(PREFIX))\n",
    "    \n",
    "print('')    \n",
    "pretty_print(pred, \"Prediction (1 inlier, -1 outlier)\")\n",
    "pretty_print(dec, \"Decison ( > 0 inlier, < 0 outlier)\")\n",
    "pretty_print(score, \"Score (lower more abnormal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model each user vs them-self\n",
    "\n",
    "This makes models per user, to see if a point is an outlier vs all their old data points.\n",
    "\n",
    "To give more weighting to 'newer' points we could add more recent ones twice for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_user_model(username, prefix='All Columns'):\n",
    "    if prefix == 'All Columns':\n",
    "        data = df\n",
    "    else:\n",
    "        cols = ['user', 'timeslice']\n",
    "        cols.extend([col for col in df if col.startswith(prefix.lower()+'_')])\n",
    "        data = df[cols]\n",
    "        \n",
    "    # Scale data\n",
    "    if SCALE_DATA:\n",
    "        numeric_col = data.columns[data.dtypes.apply(lambda c: np.issubdtype(c, np.number))]\n",
    "        scaler = StandardScaler() \n",
    "        scaled_values = scaler.fit_transform(data[numeric_col]) \n",
    "        data[numeric_col] = scaled_values\n",
    "    \n",
    "    data = data[data['user'] == username].drop('user',axis=1).drop('timeslice',axis=1)\n",
    "    if data.empty:\n",
    "        return  # skip the rest\n",
    "    sample = pd.DataFrame(data.iloc[0]).transpose()\n",
    "    features = data.drop(data.head(0).index)  \n",
    "    \n",
    "    isof = IsolationForest(behaviour='new', contamination='auto', n_jobs=-1)\n",
    "    isof.fit(features)\n",
    "    \n",
    "    isof_pred = isof.predict(sample)\n",
    "    isof_dec = isof.decision_function(sample) # > 0 normal, < 0 outlier\n",
    "    isof_score = isof.score_samples(sample) # lower more abnormal\n",
    "\n",
    "    print('%s, %s: %.2f, %.2f, %.2f' % (username, prefix, mean(isof_pred), mean(isof_dec), mean(isof_score)))\n",
    "\n",
    "    \n",
    "print(\"Prediction (1 inlier, -1 outlier), Decison ( > 0 inlier, < 0 outlier), Score (lower more abnormal)\\n\")\n",
    "\n",
    "for username in USERS:\n",
    "    for prefix in PREFIX:\n",
    "        test_user_model(username,prefix=prefix)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on models\n",
    "\n",
    "Look and see what the data looks like when condensed down to just 2 dimensions.  Done for the whole population.\n",
    "\n",
    "On the scatter plot grey is all points, red is outlier and green is inlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = 'PC 1'\n",
    "Y = 'PC 2'\n",
    "\n",
    "def draw_pca(prefix):\n",
    "    print('Calculating PCA for %s' % prefix)\n",
    "    \n",
    "    if prefix == 'All Columns':\n",
    "        data = df\n",
    "    else:\n",
    "        cols = ['user', 'timeslice']\n",
    "        cols.extend([col for col in df if col.startswith(prefix.lower()+'_')])\n",
    "        data = df[cols]\n",
    "\n",
    "    numeric_col = data.columns[data.dtypes.apply(lambda c: np.issubdtype(c, np.number))]\n",
    "    \n",
    "    # Scale features\n",
    "    #scaler = RobustScaler() # use robust scaler because we have outliers and big range\n",
    "    #scaled_values = scaler.fit_transform(data[numeric_col]) \n",
    "    #data[numeric_col] = scaled_values\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    try:\n",
    "        components = pca.fit_transform(data[numeric_col])\n",
    "    except:\n",
    "        return  # skip the rest\n",
    "    components_df = pd.DataFrame(components, columns = [X, Y])\n",
    "    data[X] = components_df[X]\n",
    "    data[Y] = components_df[Y]\n",
    "        \n",
    "    inlier_users = []\n",
    "    outlier_users = []\n",
    "    for user in USERS:\n",
    "        try:\n",
    "            val = pred[user][PREFIX.index(prefix)]\n",
    "        except:\n",
    "            continue\n",
    "        if val > 0:\n",
    "            inlier_users.append(user)\n",
    "        else:\n",
    "            outlier_users.append(user)\n",
    "    \n",
    "    inlier = data[data['user'].isin(inlier_users)]\n",
    "    outlier = data[data['user'].isin(outlier_users)]\n",
    "    \n",
    "    ax1 = data.plot(kind='scatter', x=X, y=Y, color='grey', s=1, title='PCA for %s' % prefix)\n",
    "    if not inlier.empty:\n",
    "        inlier.plot(kind='scatter', x=X, y=Y, color='green', ax=ax1, s=15)\n",
    "    if not outlier.empty:\n",
    "        outlier.plot(kind='scatter', x=X, y=Y, color='red', ax=ax1, s=15)\n",
    "    \n",
    "\n",
    "for prefix in PREFIX:\n",
    "    draw_pca(prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_user_pca(username, prefix='All Columns'):\n",
    "    if prefix == 'All Columns':\n",
    "        data = df\n",
    "    else:\n",
    "        cols = ['user', 'timeslice']\n",
    "        cols.extend([col for col in df if col.startswith(prefix.lower()+'_')])\n",
    "        data = df[cols]\n",
    "        \n",
    "    data = data[data['user'] == username]\n",
    "    \n",
    "    numeric_col = data.columns[data.dtypes.apply(lambda c: np.issubdtype(c, np.number))]\n",
    "    \n",
    "    # Scale features\n",
    "    #scaler = RobustScaler() \n",
    "    #scaled_values = scaler.fit_transform(data[numeric_col]) \n",
    "    #data[numeric_col] = scaled_values\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    try:\n",
    "        components = pca.fit_transform(data[numeric_col])\n",
    "    except:\n",
    "        return  # skip the rest\n",
    "    components_df = pd.DataFrame(components, columns = [X, Y])\n",
    "    data[X] = components_df[X]\n",
    "    data[Y] = components_df[Y]\n",
    "    \n",
    "    print(data.shape)\n",
    "    \n",
    "    data.plot(kind='scatter', x=X, y=Y, color='blue', s=10, title='PCA for %s for %s' % (username, prefix))   \n",
    "    #plt.scatter(data[X], data[Y])    \n",
    "\n",
    "for username in USERS:\n",
    "     test_user_pca(username, prefix=\"General\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
