{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CP4S Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install sklearn\n",
    "!pip install git+https://github.com/IBM/ibm-cp4s-client.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cp4s.client import CP4S\n",
    "from os import environ as env\n",
    "ac = CP4S(url=env['CP4S_API_ENDPOINT'],\n",
    "         username=env['CP4S_APIKEY_USERNAME'],\n",
    "         password=env['CP4S_APIKEY_PASSWORD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = ac.search_df(\n",
    "    query=\"[ipv4-addr:value = '127.0.0.1']\",\n",
    "    configs=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.dates as md\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# method to extract child count\n",
    "def getChildCount(row):\n",
    "  value=0\n",
    "  for x in new_df.index:\n",
    "    if row['process_pid']==new_df['process_parent_pid'][x]:\n",
    "       value=value+1\n",
    "  return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop and rename\n",
    "File1=mdf.drop(columns=['domain_name','process_binary_name','process_creator_user_ref','process_opened_connection_binary_hashes_md5','process_opened_connection_binary_name','process_opened_connection_command_line','process_opened_connection_created','process_opened_connection_creator_user_ref', 'process_opened_connection_name','process_opened_connection_opened_connection_','process_opened_connection_parent_name','process_opened_connection_parent_pid', 'process_opened_connection_pid','process_opened_connection_src_addr','process_parent_binary_hashes_md5', 'process_parent_binary_name'])\n",
    "new_df=File1.rename(columns={'process_creator_user_user_id':'proc_username','process_opened_connection_count':'proc_netconn_count','process_parent_name':'parent_name','user_account_user_id':'proc_hostname','process_binary_hashes_md5':'proc_md5','process_command_line':'proc_cmdline'})\n",
    "\n",
    "# add child count and duration\n",
    "new_df['proc_child_count'] = new_df.apply(getChildCount, axis=1)\n",
    "new_df['duration']=(pd.to_datetime(new_df['last_observed']))-(pd.to_datetime(new_df['first_observed']))\n",
    "\n",
    "# drop more\n",
    "new_df=new_df.drop(columns=['created_by_ref','first_observed','id','last_observed','network_traffic_src_addr','process_created','tod','cmd_len', 'network_traffic_dst_addr' ,'process_parent_pid', 'process_pid' ,'proc_hostname','process_opened_connection_dst_addr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary to store count of unique txts in each column\n",
    "def CreateCountDict():\n",
    "  FinalDict={}\n",
    "  cols=['proc_username','proc_cmdline','proc_md5','parent_name','proc_child_count','proc_netconn_count','process_name']\n",
    "  for x in cols:\n",
    "    dict1=(pd.DataFrame(new_df[x].value_counts())).to_dict()\n",
    "    FinalDict.update(dict1)\n",
    "  return FinalDict\n",
    "\n",
    "# get the desired representation of data\n",
    "def CountNormRepresntation(ProcessData):\n",
    "  ProcessDataC=ProcessData.copy(deep=False)\n",
    "  totalLength=len(ProcessDataC.index)\n",
    "  cols=['proc_username','proc_cmdline','proc_md5','parent_name','proc_child_count','proc_netconn_count','process_name']\n",
    "  for x in cols:\n",
    "      y=ProcessDataC[x].unique()\n",
    "      for i in y:\n",
    "          ProcessDataC[x]=ProcessDataC[x].replace(i,FinalDict_x[x][i])\n",
    "  return ProcessDataC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace unknown by label Unk\n",
    "new_df=new_df.fillna(\"UnK\")\n",
    "\n",
    "# create dictionary and final data form\n",
    "FinalDict_x=CreateCountDict()\n",
    "ProcessDataC=CountNormRepresntation(new_df)\n",
    "\n",
    "# normalize the data\n",
    "cols_to_norm = ['proc_username','proc_cmdline','proc_md5','parent_name','process_name','proc_netconn_count','proc_child_count']\n",
    "ProcessDataC[cols_to_norm] = ProcessDataC[cols_to_norm].apply(lambda x: (x - x.mean()) / (x.std()))\n",
    "\n",
    "# remove the cols are not adding any info as same value\n",
    "ProcessDataC=ProcessDataC.drop(columns=['proc_netconn_count','proc_child_count','duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca for visualisation\n",
    "pca = PCA(n_components=2)\n",
    "datanew = pca.fit_transform(ProcessDataC)\n",
    "\n",
    "# standardize these 2 new features\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(datanew)\n",
    "datanew = pd.DataFrame(np_scaled)\n",
    "\n",
    "# elbow method to decide on number of clusters\n",
    "from sklearn.cluster import KMeans\n",
    "n_cluster = range(1, 11)\n",
    "kmeans = [KMeans(n_clusters=i).fit(datanew) for i in n_cluster]\n",
    "scores = [kmeans[i].score(datanew) for i in range(len(kmeans))]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(n_cluster, scores)\n",
    "plt.show()\n",
    "\n",
    "ProcessDataC['cluster'] = kmeans[1].predict(datanew)\n",
    "print(ProcessDataC['cluster'].value_counts())\n",
    "\n",
    "ProcessDataC['principal_feature1'] = datanew[0]\n",
    "ProcessDataC['principal_feature2'] = datanew[1]\n",
    "\n",
    "# plot the clusters\n",
    "fig, ax = plt.subplots()\n",
    "colors = {0:'red', 1:'blue'}\n",
    "ax.scatter(ProcessDataC['principal_feature1'],ProcessDataC['principal_feature2'],c=ProcessDataC[\"cluster\"].apply(lambda x: colors[x]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=new_df.loc[ProcessDataC[\"cluster\"] == 0,:]\n",
    "x['proc_cmdline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in cluster 0\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in cluster 1\n",
    "x=new_df.loc[ProcessDataC[\"cluster\"] == 1,:]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a CP4S Case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 with local CP4S env",
   "language": "python",
   "name": "python3_local_cp4s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
